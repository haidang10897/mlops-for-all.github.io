<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>API Deployment on</title><link>https://mlops-for-all.github.io/en/docs/api-deployment/</link><description>Recent content in API Deployment on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 06 Oct 2020 08:48:23 +0000</lastBuildDate><atom:link href="https://mlops-for-all.github.io/en/docs/api-deployment/index.xml" rel="self" type="application/rss+xml"/><item><title>1. What is API Deployment?</title><link>https://mlops-for-all.github.io/en/docs/api-deployment/what-is-api-deployment/</link><pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate><guid>https://mlops-for-all.github.io/en/docs/api-deployment/what-is-api-deployment/</guid><description>API Deployment란? # 머신러닝 모델을 학습한 뒤에는 어떻게 사용해야 할까요?
머신러닝을 학습할 때는 더 높은 성능의 모델이 나오기를 기대하지만, 학습된 모델을 사용하여 추론을 할 때는 빠르고 쉽게 추론 결과를 받아보고 싶을 것입니다.
모델의 추론 결과를 확인하고자 할 때 주피터 노트북이나 파이썬 스크립트를 통해 학습된 모델을 로드한 뒤 추론할 수 있습니다.
그렇지만 이런 방법은 모델이 클수록 모델을 불러오는 데 많은 시간을 소요하게 되어서 비효율적입니다. 또한 이렇게 이용하면 많은 사람이 모델을 이용할 수 없고 학습된 모델이 있는 환경에서밖에 사용할 수 없습니다.</description></item><item><title>2. Deploy SeldonDeployment</title><link>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-iris/</link><pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate><guid>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-iris/</guid><description>SeldonDeployment를 통해 배포하기 # 이번에는 학습된 모델이 있을 때 SeldonDeployment를 통해 API Deployment를 해보겠습니다. SeldonDeployment는 쿠버네티스(Kubernetes)에 모델을 REST/gRPC 서버의 형태로 배포하기 위해 정의된 CRD(CustomResourceDefinition)입니다.
1. Prerequisites # SeldonDeployment 관련된 실습은 seldon-deploy라는 새로운 네임스페이스(namespace)에서 진행하도록 하겠습니다. 네임스페이스를 생성한 뒤, seldon-deploy를 현재 네임스페이스로 설정합니다.
kubectl create namespace seldon-deploy kubectl config set-context --current --namespace=seldon-deploy 2. 스펙 정의 # SeldonDeployment를 배포하기 위한 yaml 파일을 생성합니다. 이번 페이지에서는 공개된 iris model을 사용하도록 하겠습니다. 이 iris model은 sklearn 프레임워크를 통해 학습되었기 때문에 SKLEARN_SERVER를 사용합니다.</description></item><item><title>3. Seldon Monitoring</title><link>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-pg/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-pg/</guid><description>Grafana &amp;amp; Prometheus # 이제, 지난 페이지에서 생성했던 SeldonDeployment 로 API Request 를 반복적으로 수행해보고, 대시보드에 변화가 일어나는지 확인해봅니다.
대시보드 # 앞서 생성한 대시보드를 포트 포워딩합니다.
kubectl port-forward svc/seldon-core-analytics-grafana -n seldon-system 8090:80 API 요청 # 앞서 생성한 Seldon Deployment에 요청을 반복해서 보냅니다.
curl -X POST http://$NODE_IP:$NODE_PORT/seldon/seldon-deploy/sklearn/api/v1.0/predictions \ -H &amp;#39;Content-Type: application/json&amp;#39; \ -d &amp;#39;{ &amp;#34;data&amp;#34;: { &amp;#34;ndarray&amp;#34;: [[1,2,3,4]] } }&amp;#39; 그리고 그라파나 대시보드를 확인하면 다음과 같이 Global Request Rate 이 0 ops 에서 순간적으로 상승하는 것을 확인할 수 있습니다.</description></item><item><title>4. Seldon Fields</title><link>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-fields/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-fields/</guid><description>How Seldon Core works? # Seldon Core가 API 서버를 생성하는 과정을 요약하면 다음과 같습니다.
initContainer는 모델 저장소에서 필요한 모델을 다운로드 받습니다. 다운로드받은 모델을 container로 전달합니다. container는 전달받은 모델을 감싼 API 서버를 실행합니다. 생성된 API 서버 주소로 API를 요청하여 모델의 추론 값을 받을 수 있습니다. SeldonDeployment Spec # Seldon Core를 사용할 때, 주로 사용하게 되는 커스텀 리소스인 SeldonDeployment를 정의하는 yaml 파일은 다음과 같습니다.
apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: seldon-example namespace: kubeflow-user-example-com spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} initContainers: - name: model-initializer image: gcr.</description></item><item><title>5. Model from MLflow</title><link>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-mlflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-mlflow/</guid><description>Model from MLflow # 이번 페이지에서는 MLflow Component에서 저장된 모델을 이용해 API를 생성하는 방법에 대해서 알아보겠습니다.
Secret # initContainer가 minio에 접근해서 모델을 다운로드받으려면 credentials가 필요합니다. minio에 접근하기 위한 credentials는 다음과 같습니다.
apiVersion: v1 type: Opaque kind: Secret metadata: name: seldon-init-container-secret namespace: kubeflow-user-example-com data: AWS_ACCESS_KEY_ID: bWluaW8K= AWS_SECRET_ACCESS_KEY: bWluaW8xMjM= AWS_ENDPOINT_URL: aHR0cDovL21pbmlvLm1ha2luYXJvY2tzLmFp USE_SSL: ZmFsc2U= AWS_ACCESS_KEY_ID 의 입력값은 minio입니다. 다만 secret의 입력값은 인코딩된 값이여야 되기 때문에 실제로 입력되는 값은 다음을 수행후 나오는 값이어야 합니다.</description></item><item><title>6. Multi Models</title><link>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-children/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mlops-for-all.github.io/en/docs/api-deployment/seldon-children/</guid><description>Multi Models # 앞서 설명했던 방법들은 모두 단일 모델을 대상으로 했습니다.
이번 페이지에서는 여러 개의 모델을 연결하는 방법에 대해서 알아봅니다.
Pipeline # 우선 모델을 2개를 생성하는 파이프라인을 작성하겠습니다.
모델은 앞서 사용한 SVC 모델에 StandardScaler를 추가하고 저장하도록 하겠습니다.
from functools import partial import kfp from kfp.components import InputPath, OutputPath, create_component_from_func @partial( create_component_from_func, packages_to_install=[&amp;#34;pandas&amp;#34;, &amp;#34;scikit-learn&amp;#34;], ) def load_iris_data( data_path: OutputPath(&amp;#34;csv&amp;#34;), target_path: OutputPath(&amp;#34;csv&amp;#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.</description></item></channel></rss>